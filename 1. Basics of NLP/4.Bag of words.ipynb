{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to get the Bag of words\n",
    "    Step 1 : Lowering the sentances\n",
    "    Step 2 : Stemming\n",
    "    Step 3 : Lemmatization\n",
    "    Step 4 : Counting Frequency of each word (Creating histogram for wach word)\n",
    "    Step 5 : Converting bag of words to vectors\n",
    "\n",
    "### Example :-\n",
    "    \n",
    "    He is boy.                  he is boy                boy\n",
    "    She is good girl.           she is good girl         good,girl\n",
    "    \n",
    "    \n",
    "    Bag of Words :-\n",
    "    \n",
    "    word  count\n",
    "    boy     1                    \n",
    "    good    1\n",
    "    girl    1\n",
    "    \n",
    "                        good      boy      girl\n",
    "    sent 1               0         1        0\n",
    "    sent 2               1         0        1\n",
    "    \n",
    "    This how we convert the text to Numerical representation.\n",
    "\n",
    "## Disadvantages of Bag of Words\n",
    "\n",
    "### 1.We are representating all the words by 1 and 0.\n",
    "\n",
    "\n",
    "### 2.Hence we cannot determine which word is used to identify the sentance is either +ve or -ve.\n",
    "\n",
    "### 3.To solve this problem, we uses TFIDF (Term Frequency And Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining variable paragraph\n",
    "\n",
    "paragraph = \"\"\"Natural Language Processing (NLP) is a subfield of computer science, artificial intelligence, information engineering, and human-computer interaction. \n",
    "                This field focuses on how to program computers to process and analyze large amounts of natural language data. \n",
    "                It is difficult to perform as the process of reading and understanding languages is far more complex than it seems at first glance. \n",
    "                Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. \n",
    "                One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the text (lowering)\n",
    "import re\n",
    "\n",
    "#PorterStemmer is used to get the word stem\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#WordNetLemmatizer is used for limitization to the word stem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#stopwords helps us to remove the words like 'for', 'then', 'from', 'and' which are repeting again and again\n",
    "#which does not put much value to identify sentance\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating objects for PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#creating objects for WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#tokenizing sentances\n",
    "sentances = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "#After cleaning the text we are going to store final result to corpus\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process nlp subfield comput scienc artifici intellig inform engin human comput interact',\n",
       " 'field focus program comput process analyz larg amount natur languag data',\n",
       " 'difficult perform process read understand languag far complex seem first glanc',\n",
       " 'token process token split string text list token',\n",
       " 'one think token part like word token sentenc sentenc token paragraph']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleaning text stemming\n",
    "for i in range(len(sentances)):\n",
    "    \n",
    "    #replacing all other information like '. ,'  with space\n",
    "    review = re.sub('[^a-zA-Z]',' ', sentances[i])      \n",
    "    \n",
    "    #lowering the sentances\n",
    "    review = review.lower()\n",
    "    \n",
    "    #spliting the sentance to get the words\n",
    "    review = review.split()\n",
    "    \n",
    "    #list comprehension (stemming)\n",
    "    review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    #joining all the list of words to review\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    #appending all the words to corpus\n",
    "    corpus.append(review)\n",
    "    \n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are not getting meaning full words hence we use Lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing nlp subfield computer science artificial intelligence information engineering human computer interaction',\n",
       " 'field focus program computer process analyze large amount natural language data',\n",
       " 'difficult perform process reading understanding language far complex seems first glance',\n",
       " 'tokenization process tokenizing splitting string text list token',\n",
       " 'one think token part like word token sentence sentence token paragraph']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words are not clear hence we do lemmatization.\n",
    "\n",
    "#Cleaning text lemmatization\n",
    "for i in range(len(sentances)):\n",
    "    \n",
    "    #replacing all other information like '. ,'  with space\n",
    "    review = re.sub('[^a-zA-Z]',' ', sentances[i])      \n",
    "    \n",
    "    #lowering the sentances\n",
    "    review = review.lower()\n",
    "    \n",
    "    #spliting the sentance to get the words\n",
    "    review = review.split()\n",
    "    \n",
    "    #list comprehension (stemming)\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    #joining all the list of words to review\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    #appending all the words to corpus\n",
    "    corpus.append(review)\n",
    "    \n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#creating object\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "\n",
    "#creating matrix\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 3, 0, 0, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the Bag of Words\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result :- We get the resultant matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
